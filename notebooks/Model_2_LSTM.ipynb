{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "_-tkIHednLFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing LSTM-Based Music Generation Model\n",
        "The model is tailored to understand and replicate the complexities of musical compositions.\n",
        "\n",
        "- This model will be compiled with sparse categorical crossentropy as the loss function, which is appropriate for this multi-class classification problem.\n",
        "\n",
        "- Adam optimizer will be used for efficient training.\n"
      ],
      "metadata": {
        "id": "miUeGeiJAw5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation\n",
        "Adjusting note representation to include note duration and time offsets."
      ],
      "metadata": {
        "id": "s7OiSdrmRt4D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-ATcHWKkMve"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from music21 import converter, instrument, note, chord\n",
        "\n",
        "def read_midi(file):\n",
        "    print(\"Loading Music File:\", file)\n",
        "    notes = []\n",
        "\n",
        "    midi = converter.parse(file)\n",
        "    parts = instrument.partitionByInstrument(midi)\n",
        "    relevant_parts = parts.parts if parts else [midi]\n",
        "\n",
        "    for part in relevant_parts:\n",
        "        if 'Violin' in str(part.getInstrument()) or 'Violin' in str(part.partName):\n",
        "            for element in part.recurse():\n",
        "                if isinstance(element, note.Note):\n",
        "                    notes.append((str(element.pitch), element.duration.quarterLength, element.offset))\n",
        "                elif isinstance(element, chord.Chord):\n",
        "                    notes.append(('.'.join(str(n) for n in element.normalOrder), element.duration.quarterLength, element.offset))\n",
        "                elif isinstance(element, note.Rest):\n",
        "                    notes.append(('rest', element.duration.quarterLength, element.offset))\n",
        "\n",
        "    return notes\n",
        "\n",
        "\n",
        "path = '/content/gdrive/MyDrive/Violin_Comp_Data/midi_150/'\n",
        "files = [i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
        "notes_array = [read_midi(os.path.join(path, file)) for file in files]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding each unique note to an integer"
      ],
      "metadata": {
        "id": "kFSRzbE2WuO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten\n",
        "all_notes = [note for sequence in notes_array for note in sequence]\n",
        "\n",
        "# Mapping from notes to integers\n",
        "note_to_int = {note: i for i, note in enumerate(sorted(set(all_notes)))}\n",
        "\n",
        "# Encode sequences\n",
        "input_sequences = []\n",
        "output_notes = []\n",
        "no_of_timesteps = 32\n",
        "\n",
        "for notes in notes_array:\n",
        "    for i in range(len(notes) - no_of_timesteps):\n",
        "        input_seq = notes[i:i + no_of_timesteps]\n",
        "        output_note = notes[i + no_of_timesteps]\n",
        "        input_sequences.append([note_to_int[note] for note in input_seq])\n",
        "        output_notes.append(note_to_int[output_note])\n",
        "\n",
        "x_seq = np.array(input_sequences)\n",
        "y_seq = np.array(output_notes)\n"
      ],
      "metadata": {
        "id": "z3yOeDPRbIxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initiating Train-Test Split & Reshaping Input for LSTM Model"
      ],
      "metadata": {
        "id": "-FNWACczUx9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(x_seq, y_seq, test_size=0.2, random_state=13)\n",
        "x_tr = np.reshape(x_tr, (x_tr.shape[0], no_of_timesteps, 1))\n",
        "x_val = np.reshape(x_val, (x_val.shape[0], no_of_timesteps, 1))\n"
      ],
      "metadata": {
        "id": "rORuqgonXcTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM Model Architecture\n",
        "Trying a LSTM-based architecture for better sequence generation."
      ],
      "metadata": {
        "id": "OohVKtacSrIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(no_of_timesteps, 1), return_sequences=True))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(len(note_to_int), activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "qa49DYYvmYQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Checkpoint"
      ],
      "metadata": {
        "id": "lffS56B88E7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "mc = ModelCheckpoint('best_model_lstm1.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)"
      ],
      "metadata": {
        "id": "sBjqEA7WmYOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Model"
      ],
      "metadata": {
        "id": "gFMZPsfo8CIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_tr, y_tr, epochs=50, batch_size=128, validation_data=(x_val, y_val), callbacks=[mc])"
      ],
      "metadata": {
        "id": "wRl6w_DFmYMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Best Model"
      ],
      "metadata": {
        "id": "ZW8Qn8-T7vKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('best_model_lstm1.h5')"
      ],
      "metadata": {
        "id": "ltnjLjqfmYIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Music Predictions"
      ],
      "metadata": {
        "id": "c1mdFZUX7zC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_music(model, start_sequence, length=50):\n",
        "    prediction_output = []\n",
        "\n",
        "    # Ensure start_sequence is correctly formatted\n",
        "    start_sequence_formatted = np.array([note_to_int[note] for note in start_sequence])\n",
        "\n",
        "    for note_index in range(length):\n",
        "        prediction_input = np.reshape(start_sequence_formatted, (1, len(start_sequence_formatted), 1))\n",
        "        prob = model.predict(prediction_input)[0]\n",
        "        index = np.random.choice(range(len(prob)), p=prob)\n",
        "        predicted_note = x_int_to_note[index]  # Use the inverse mapping\n",
        "        prediction_output.append(predicted_note)\n",
        "\n",
        "        # Update start_sequence_formatted for the next prediction\n",
        "        start_sequence_formatted = np.append(start_sequence_formatted, [index])[1:]\n",
        "\n",
        "    return prediction_output\n",
        "\n",
        "# Create the inverse mapping from integers back to note tuples\n",
        "x_int_to_note = dict((number, note) for note, number in note_to_int.items())"
      ],
      "metadata": {
        "id": "wF87tLOZmYGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert Back to MIDI"
      ],
      "metadata": {
        "id": "NqQCCHlE75Va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from music21 import stream, instrument, note, chord, midi\n",
        "\n",
        "def midi_number_to_note_name(midi_number):\n",
        "    # Converts a MIDI note number to a note name\n",
        "    return midi.translate.pitchToNoteName(midi_number)\n",
        "\n",
        "def convert_to_midi(prediction_output):\n",
        "    midi_stream = stream.Stream()\n",
        "    midi_stream.append(instrument.Violin())\n",
        "\n",
        "    offset = 0\n",
        "    for note_info in prediction_output:\n",
        "        note_name = note_info[0]\n",
        "\n",
        "        # Convert MIDI note numbers to note names\n",
        "        if note_name.isdigit():\n",
        "            note_name = midi_number_to_note_name(int(note_name))\n",
        "\n",
        "        # Create note or rest\n",
        "        if note_name != 'rest':\n",
        "            new_note = note.Note(note_name)\n",
        "        else:\n",
        "            new_note = note.Rest()\n",
        "\n",
        "        new_note.duration.quarterLength = note_info[1]\n",
        "        new_note.offset = offset\n",
        "        new_note.storedInstrument = instrument.Violin()\n",
        "        midi_stream.append(new_note)\n",
        "        offset += note_info[2]\n",
        "\n",
        "    midi_stream.write('midi', fp='lstm_music1.mid')"
      ],
      "metadata": {
        "id": "8JzJHhffgz5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from music21 import pitch\n",
        "\n",
        "# Randomly select a starting sequence from x_val\n",
        "random_index = np.random.randint(0, len(x_val))\n",
        "start_sequence = x_val[random_index]\n",
        "\n",
        "# Since start_sequence is currently encoded as integers, decode it back to note information\n",
        "start_sequence_decoded = [x_int_to_note[note] for note in start_sequence.flatten()]\n",
        "\n",
        "# Generate music based on the starting sequence\n",
        "prediction_output = generate_music(model, start_sequence_decoded)\n",
        "convert_to_midi(prediction_output)\n"
      ],
      "metadata": {
        "id": "p5c3cXUF_eiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MIDI to WAV Conversion using FluidSynth\n"
      ],
      "metadata": {
        "id": "g1UtNvpeOUmC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qf6O6Wsh_3s"
      },
      "outputs": [],
      "source": [
        "!apt install -y fluidsynth\n",
        "!pip install midi2audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoAkgDnvieiU"
      },
      "outputs": [],
      "source": [
        "from midi2audio import FluidSynth\n",
        "\n",
        "# Initialize FluidSynth with a sound font\n",
        "fs = FluidSynth('/content/gdrive/MyDrive/Violin_Comp_Data/soundfonts/Acro_Violins.sf2')\n",
        "\n",
        "# Convert MIDI to WAV\n",
        "fs.midi_to_audio('lstm_music1.mid', 'lstm1_acro.wav')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Music Generation: Pitch and Rhythm Consistency\n",
        "Together, pitch and rhythm consistency form two fundamental pillars of music that determine its overall quality and appeal. By evaluating these aspects, we can gauge the success of our music generation models in producing compositions that are not just technically sound but also musically coherent and enjoyable.\n",
        "\n"
      ],
      "metadata": {
        "id": "p1sz7v9I4N9q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_86mJgzFPGBO"
      },
      "source": [
        "### Pitch Consistency\n",
        "Pitch consistency can be evaluated by extracting the pitch from the audio and then analyzing its stability and variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xB9ZaSxPJy6"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "\n",
        "def calculate_pitch_consistency(audio, sr):\n",
        "    # Extract pitch\n",
        "    pitches, magnitudes = librosa.piptrack(y=audio, sr=sr)\n",
        "    # Select the predominant pitch at each frame\n",
        "    predominant_pitches = [pitches[magnitudes[:, t].argmax(), t] for t in range(pitches.shape[1])]\n",
        "    predominant_pitches = np.array(predominant_pitches)\n",
        "\n",
        "    # Calculate variance\n",
        "    pitch_variance = np.var(predominant_pitches)\n",
        "    return pitch_variance\n",
        "\n",
        "# Baseline WaveNet\n",
        "audio, sr = librosa.load('lstm1_acro.wav')\n",
        "pitch_variance = calculate_pitch_consistency(audio, sr)\n",
        "print(\"First Iteration LSTM Pitch Variance:\", pitch_variance)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO7oaEcUPM32"
      },
      "source": [
        "### Rhythm Consistency\n",
        "Rhythm consistency can be evaluated by analyzing the beat and tempo of the generated audio.\n",
        "- Extract Beat Information\n",
        "- Analyze Tempo Stability Over Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14BzmPGjPREg"
      },
      "outputs": [],
      "source": [
        "def calculate_rhythm_consistency(file_path):\n",
        "    audio, sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "    # Track beats\n",
        "    tempo, beats = librosa.beat.beat_track(y=audio, sr=sr)\n",
        "    beat_times = librosa.frames_to_time(beats, sr=sr)\n",
        "\n",
        "    # Calculate tempo variability\n",
        "    inter_beat_intervals = np.diff(beat_times)\n",
        "    tempo_variability = np.std(inter_beat_intervals)\n",
        "\n",
        "    return tempo, tempo_variability\n",
        "\n",
        "file_paths = ['lstm1_acro.wav']\n",
        "\n",
        "# Calculate and display rhythm consistency for each file\n",
        "for i, file_path in enumerate(file_paths):\n",
        "    tempo, tempo_variability = calculate_rhythm_consistency(file_path)\n",
        "    print(f\"File {i + 1}:\")\n",
        "    print(f\"Path: {file_path}\")\n",
        "    print(f\"Tempo: {tempo}\")\n",
        "    print(f\"Tempo Variability: {tempo_variability}\")\n",
        "    print(\"------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis of Pitch and Rhythm Consistency Results\n",
        "**Pitch Consistency:** The high pitch variance is an area of concern.\n",
        "\n",
        "**Rhythm Consistency:** The results here are more promising. The consistency in tempo suggests the model is capturing the rhythmic aspect of the music well.\n",
        "\n",
        "## Conclusion\n",
        "While the rhythm consistency aspect of this model appears solid, the pitch consistency needs improvement. Next, I will increase the complexity of the LSTM model."
      ],
      "metadata": {
        "id": "pxTGJ3nFFQ3m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proceed to 'Model_3_Complex_LSTM.ipynb'\n",
        "``` bash\n",
        "├── AI_Violinist_Intro.ipynb                <- Data capture/project overview\n",
        "├── Model_1_WaveNet.ipynb                   <- Baseline/WaveNet Models\n",
        "├── Model_2_LSTM.ipynb                      <- First LSTM Model\n",
        "├── Model_3__Complex_LSTM.ipynb             <- Second LSTM Model\n",
        "├── Visual_Analysis_Model_Comparison.ipynb  <- Model Evaluation\n",
        "├── Pretrained_Model_Jukebox.ipynb          <- Generating Final Music\n",
        "└── Failed_Models_Spectrograms.ipynb        <- Failed attempts\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "qoZ5baL28WkO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m2g9WWDEGSUX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
