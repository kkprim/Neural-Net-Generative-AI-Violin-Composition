{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "_-tkIHednLFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling\n",
        "Before proceeding to the generative models, my audio data will need to be converted to MIDI files for processing, which requires several steps for optimization.\n"
      ],
      "metadata": {
        "id": "ftESYTvgyin-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install crepe"
      ],
      "metadata": {
        "id": "w0t0839b97oO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install essentia"
      ],
      "metadata": {
        "id": "OFQcYDyJ-O8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pretty_midi"
      ],
      "metadata": {
        "id": "IVzJ1hKBqKzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import random\n",
        "import essentia.standard as es\n",
        "import pretty_midi\n",
        "import crepe\n",
        "from scipy.io import wavfile\n",
        "import os\n",
        "import shutil\n",
        "import random"
      ],
      "metadata": {
        "id": "q2ncyJaO33Q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Music Information Retrieval"
      ],
      "metadata": {
        "id": "1E8qKJ12xV3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pitch Detection with CREPE\n",
        "CREPE (Convolutional Representation for Pitch Estimation) is a deep learning model designed for pitch detection. This will read each WAV file in my audio folder and use CREPE to predict the pitch at each time step."
      ],
      "metadata": {
        "id": "poTMbiLMitJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import crepe\n",
        "from scipy.io import wavfile\n",
        "\n",
        "audio_folder = '/content/gdrive/MyDrive/Violin_Comp_Data/sample_music_files_150'\n",
        "pitch_data = {}\n",
        "\n",
        "for filename in os.listdir(audio_folder):\n",
        "    if filename.endswith('.wav'):\n",
        "        file_path = os.path.join(audio_folder, filename)\n",
        "        sr, audio = wavfile.read(file_path)\n",
        "        time, frequency, confidence, activation = crepe.predict(audio, sr, viterbi=True)\n",
        "\n",
        "        pitch_data[filename] = {\n",
        "            'time': time,\n",
        "            'frequency': frequency,\n",
        "            'confidence': confidence\n",
        "        }"
      ],
      "metadata": {
        "id": "kghy_Ubki7Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rhythm and Tempo Analysis\n",
        "This will detect note onsets and durations."
      ],
      "metadata": {
        "id": "GFKGVxNPjIXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_rhythm(file_path):\n",
        "    y, sr = librosa.load(file_path, sr=None)\n",
        "    onset_frames = librosa.onset.onset_detect(y=y, sr=sr)\n",
        "    onset_times = librosa.frames_to_time(onset_frames, sr=sr)\n",
        "    tempogram = librosa.feature.tempogram(y=y, sr=sr)\n",
        "    tempo = librosa.beat.tempo(onset_envelope=tempogram, sr=sr)\n",
        "    return onset_times, tempo[0]"
      ],
      "metadata": {
        "id": "hW4OI89JlIUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_folder = '/content/gdrive/MyDrive/Violin_Comp_Data/sample_music_files_150'\n",
        "rhythm_data = {}\n",
        "\n",
        "for filename in os.listdir(audio_folder):\n",
        "    if filename.endswith('.wav'):\n",
        "        file_path = os.path.join(audio_folder, filename)\n",
        "        onset_times, tempo = analyze_rhythm(file_path)\n",
        "        rhythm_data[filename] = {\n",
        "            'onset_times': onset_times,\n",
        "            'tempo': tempo\n",
        "        }"
      ],
      "metadata": {
        "id": "4dttW-1HlRvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Align Pitch and Rhythm Data\n",
        "This function aligns pitch data with rhythm data. Then, it iterates over the onset times and finds the closest time in the pitch data, then extracts the corresponding pitch. I'll use a confidence threshold to filter out uncertain pitch detections."
      ],
      "metadata": {
        "id": "PoFXPQU1m9Gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def align_pitch_and_rhythm(pitch_data, rhythm_data):\n",
        "    aligned_data = []\n",
        "    for onset_time in rhythm_data['onset_times']:\n",
        "        # Find the index of the closest time in pitch_data to the onset_time\n",
        "        closest_index = np.argmin(np.abs(pitch_data['time'] - onset_time))\n",
        "        pitch = pitch_data['frequency'][closest_index]\n",
        "        confidence = pitch_data['confidence'][closest_index]\n",
        "\n",
        "        # Only consider pitches with a confidence above a certain threshold\n",
        "        if confidence > 0.8:\n",
        "            aligned_data.append((onset_time, pitch))\n",
        "\n",
        "    return aligned_data"
      ],
      "metadata": {
        "id": "JhKGilc7m8vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create MIDI Files\n",
        "With the aligned pitch and rhythm data, I will now create MIDI files. For simplicity, I will set each note's duration to the interval between its onset time and the next onset time."
      ],
      "metadata": {
        "id": "p-zzm7izngU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_midi(aligned_data, output_file='output.mid'):\n",
        "    midi = pretty_midi.PrettyMIDI()\n",
        "    violin = pretty_midi.Instrument(program=pretty_midi.instrument_name_to_program('Violin'))\n",
        "\n",
        "    for i, (start_time, pitch) in enumerate(aligned_data):\n",
        "        end_time = aligned_data[i+1][0] if i+1 < len(aligned_data) else start_time + 1\n",
        "        note_number = pretty_midi.hz_to_note_number(pitch)\n",
        "\n",
        "        # Ensure note_number is an integer and within MIDI range\n",
        "        note_number = int(round(note_number))\n",
        "        note_number = max(0, min(note_number, 127))\n",
        "\n",
        "        note = pretty_midi.Note(\n",
        "            velocity=100,\n",
        "            pitch=note_number,\n",
        "            start=start_time,\n",
        "            end=end_time\n",
        "        )\n",
        "        violin.notes.append(note)\n",
        "\n",
        "    midi.instruments.append(violin)\n",
        "    midi.write(output_file)\n",
        "\n",
        "# Directory to store all MIDI files\n",
        "midi_folder = '/content/gdrive/MyDrive/Violin_Comp_Data/midi_150'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(midi_folder):\n",
        "    os.makedirs(midi_folder)"
      ],
      "metadata": {
        "id": "su8dJ0QjnirS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply to All Files"
      ],
      "metadata": {
        "id": "dZEZ60GWnrs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for filename in os.listdir(audio_folder):\n",
        "    if filename.endswith('.wav'):\n",
        "        pitch_file = pitch_data[filename]\n",
        "        rhythm_file = rhythm_data[filename]\n",
        "        aligned_data = align_pitch_and_rhythm(pitch_file, rhythm_file)\n",
        "        midi_file = f\"{filename.split('.')[0]}.mid\"\n",
        "        create_midi(aligned_data, output_file=midi_file)"
      ],
      "metadata": {
        "id": "HWecGDWnnoyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation\n",
        "Defining a function for reading the MIDI files, which will give an array of notes and chords in the audio."
      ],
      "metadata": {
        "id": "SvTdE2Ivxy5S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-ATcHWKkMve"
      },
      "outputs": [],
      "source": [
        "from music21 import converter, instrument, note, chord\n",
        "\n",
        "def read_midi(file):\n",
        "    #print(\"Loading Music File:\", file)\n",
        "    notes = []\n",
        "\n",
        "    # Parsing a MIDI file\n",
        "    midi = converter.parse(file)\n",
        "\n",
        "    # Grouping based on different instruments\n",
        "    parts = instrument.partitionByInstrument(midi)\n",
        "\n",
        "    # Check if there are parts, if not, consider the whole stream\n",
        "    if parts:\n",
        "        relevant_parts = parts.parts\n",
        "    else:\n",
        "        relevant_parts = [midi]\n",
        "\n",
        "    # Looping over all parts to find violin\n",
        "    for part in relevant_parts:\n",
        "        if isinstance(part.getInstrument(), instrument.Violin) or \"Violin\" in part.partName:\n",
        "            for element in part.recurse():\n",
        "                # Note or chord handling\n",
        "                if isinstance(element, note.Note):\n",
        "                    notes.append((str(element.pitch), element.duration.quarterLength, element.offset))\n",
        "                elif isinstance(element, chord.Chord):\n",
        "                    notes.append(('.'.join(str(n) for n in element.normalOrder), element.duration.quarterLength, element.offset))\n",
        "\n",
        "    return np.array(notes)\n",
        "\n",
        "\n",
        "path = '/content/gdrive/MyDrive/Violin_Comp_Data/midi_150/'\n",
        "\n",
        "# Read all the filenames\n",
        "files = [i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
        "\n",
        "# Reading each MIDI file\n",
        "notes_array = [read_midi(path + i) for i in files]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Type of notes_array:\", type(notes_array))\n",
        "print(\"First two elements of notes_array:\", notes_array[:2])\n"
      ],
      "metadata": {
        "id": "XHsyErRcpG0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flatten Function and Notes Extraction\n",
        "The following function is to flatten a nested list or array structure. After defining the function, I'll use it to flatten `notes_array` into a single list `notes_flat` for model processing.\n"
      ],
      "metadata": {
        "id": "1kMGC9QUzn6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten(array):\n",
        "    for item in array:\n",
        "        if isinstance(item, (list, np.ndarray)):\n",
        "            yield from flatten(item)\n",
        "        else:\n",
        "            yield item\n",
        "\n",
        "notes_flat = list(flatten(notes_array))\n"
      ],
      "metadata": {
        "id": "jhcik35wpQBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Identifying Unique Notes"
      ],
      "metadata": {
        "id": "rhWBcuwl0JxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_notes = list(set(notes_flat))\n",
        "print(\"Number of unique notes:\", len(unique_notes))\n"
      ],
      "metadata": {
        "id": "At7JpSVIpSyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top 20 Most Frequent Notes\n",
        "Here, I'll count the frequency of each note using the `Counter` class and then plot a bar chart for the top 20 most frequent notes. This visualization helps in understanding the distribution of notes and identifying the most commonly occurring ones.\n"
      ],
      "metadata": {
        "id": "oX51j0Bp0PUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Count the frequency of each note\n",
        "note_freq = Counter(notes_flat)\n",
        "\n",
        "# Selecting top categories\n",
        "top_n = 20\n",
        "top_items = note_freq.most_common(top_n)\n",
        "notes, frequencies = zip(*top_items)\n",
        "\n",
        "# Use numerical indices as x-values\n",
        "x_values = range(len(notes))\n",
        "\n",
        "# Create the bar chart\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(x_values, frequencies, color='blue', edgecolor='black')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Notes')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title(f'Top {top_n} Most Frequent Notes')\n",
        "\n",
        "# Set the x-axis labels to the note names\n",
        "plt.xticks(x_values, notes, rotation=90)\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Pjx50aEUtxe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note Frequency Histogram\n",
        "Now I'll focuses on creating a histogram to visualize the frequency distribution of all notes. I'll compute the frequencies using the `Counter` class and plot a histogram, providing a visual representation of how note occurrences are distributed across the dataset.\n"
      ],
      "metadata": {
        "id": "gJSEt0_40iV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing frequency of each note\n",
        "freq = dict(Counter(notes_flat))\n",
        "\n",
        "# Consider only the frequencies\n",
        "frequencies = [count for _, count in freq.items()]\n",
        "\n",
        "# Set fig size\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "# Plot\n",
        "plt.hist(frequencies, bins=20, color='blue', edgecolor='black');\n"
      ],
      "metadata": {
        "id": "ZqJKCO96rVR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frequent_notes = [note_flat for note_flat, count in freq.items() if count>=1]\n",
        "print(len(frequent_notes))"
      ],
      "metadata": {
        "id": "8b1QU5J0lzB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a Filtered Music Dataset\n",
        "Now, I'll iterate through `notes_array` to create a new dataset. This will filter and simplify the dataset based on the frequency of notes.\n"
      ],
      "metadata": {
        "id": "euyABnIY1fv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_music = []\n",
        "\n",
        "for notes in notes_array:\n",
        "    temp = []\n",
        "    for note_flat in notes:\n",
        "        for element in note_flat:\n",
        "            if element in frequent_notes:\n",
        "                temp.append(element)\n",
        "                break\n",
        "    new_music.append(temp)\n",
        "\n",
        "new_music = np.array(new_music)\n"
      ],
      "metadata": {
        "id": "nQ_b9o5zly-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing Input and Output Sequences\n",
        "Generating input and output sequences for the model. This sliding window approach prepares the dataset for time series forecasting or sequence prediction models. In this case, a sequence prediction model.\n"
      ],
      "metadata": {
        "id": "PET7Jldf1vha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_of_timesteps = 32\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for note_ in new_music:\n",
        "    for i in range(0, len(note_) - no_of_timesteps, 1):\n",
        "\n",
        "        #preparing input and output sequences\n",
        "        input_ = note_[i:i + no_of_timesteps]\n",
        "        output = note_[i + no_of_timesteps]\n",
        "\n",
        "        x.append(input_)\n",
        "        y.append(output)\n",
        "\n",
        "x=np.array(x)\n",
        "y=np.array(y)"
      ],
      "metadata": {
        "id": "hEWHLIqalyyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mapping Notes to Integers for Inputs\n",
        "For model training, I'll convert the notes in input and output sequences into integers. A unique integer is assigned to each note, creating a dictionary. Then, converting input sequences to integer sequences."
      ],
      "metadata": {
        "id": "s7uao-Ok2Ik3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_x = list(set(x.ravel()))\n",
        "x_note_to_int = dict((note_flat, number) for number, note_flat in enumerate(unique_x))\n"
      ],
      "metadata": {
        "id": "Ap_Z85Q0lywc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing input sequences\n",
        "x_seq=[]\n",
        "for i in x:\n",
        "    temp=[]\n",
        "    for j in i:\n",
        "        #assigning unique integer to every note\n",
        "        temp.append(x_note_to_int[j])\n",
        "    x_seq.append(temp)\n",
        "\n",
        "x_seq = np.array(x_seq)"
      ],
      "metadata": {
        "id": "vhnEFRF_lys7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_y = list(set(y))\n",
        "y_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_y))\n",
        "y_seq=np.array([y_note_to_int[i] for i in y])"
      ],
      "metadata": {
        "id": "Sr90U3gImYV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initiating Train-Test Split"
      ],
      "metadata": {
        "id": "mSyuwR-C3bIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(x_seq,y_seq,test_size=0.2,random_state=13)"
      ],
      "metadata": {
        "id": "gB2YUZrWmYTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a Baseline WaveNet Model\n",
        "Initializing the WaveNet model using Keras. The model includes an Embedding layer, several Convolutional layers with different dilation rates and Dropout layers for regularization, and Dense layers for the output. The model is compiled with the 'sparse_categorical_crossentropy' loss function and the 'adam' optimizer."
      ],
      "metadata": {
        "id": "c1ScmpiE4S4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras.callbacks import *\n",
        "import keras.backend as K\n",
        "\n",
        "K.clear_session()\n",
        "model = Sequential()\n",
        "\n",
        "#embedding layer\n",
        "model.add(Embedding(len(unique_x), 100, input_length=32,trainable=True))\n",
        "\n",
        "model.add(Conv1D(64,3, padding='causal',activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(MaxPool1D(2))\n",
        "\n",
        "model.add(Conv1D(128,3,activation='relu',dilation_rate=2,padding='causal'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(MaxPool1D(2))\n",
        "\n",
        "model.add(Conv1D(256,3,activation='relu',dilation_rate=4,padding='causal'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(MaxPool1D(2))\n",
        "\n",
        "#model.add(Conv1D(256,5,activation='relu'))\n",
        "model.add(GlobalMaxPool1D())\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(len(unique_y), activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "qa49DYYvmYQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Up Model Checkpointing\n",
        "This will save the model whenever there is an improvement in the validation loss during training, ensuring that I always have the best performing model saved."
      ],
      "metadata": {
        "id": "fvv5QnhQ55lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mc=ModelCheckpoint('best_model3.h5', monitor='val_loss', mode='min', save_best_only=True,verbose=1)"
      ],
      "metadata": {
        "id": "sBjqEA7WmYOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Model\n",
        "The model is trained with a batch size of 128 for 50 epochs. Validation data is used to monitor the model's performance on unseen data. The model checkpoint callback is used to save the best model during training.\n"
      ],
      "metadata": {
        "id": "8OjuOtC86L0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(np.array(x_tr),np.array(y_tr),batch_size=128,epochs=50, validation_data=(np.array(x_val),np.array(y_val)),verbose=1, callbacks=[mc])"
      ],
      "metadata": {
        "id": "wRl6w_DFmYMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis of Model Training Performance (Loss and Accuracy)\n",
        "\n",
        "#### Overview\n",
        "The training of this model over 50 epochs shows a trend in both loss (a measure of error) and accuracy (a measure of correct predictions). This dual-metric evaluation provides a more comprehensive view of the model's performance.\n",
        "\n",
        "#### Observations\n",
        "- **Best Model Consideration**: The model saved at epoch 34 is likely the best model to use because it has the lowest validation loss.\n",
        "  - Loss = 2.29707\n",
        "  - Accuracy = .6438\n",
        "- **Potential Overfitting**: Towards the end, the model shows signs of overfitting, as indicated by the plateauing and slight increases in validation loss, despite the training loss continuing to decrease.\n",
        "\n",
        "#### Conclusion\n",
        "The model has demonstrated a capacity to learn and improve both in terms of reducing error and increasing prediction accuracy. However, the fluctuations and final plateau in validation metrics suggest a need for further tuning or regularization to enhance model generalization.\n"
      ],
      "metadata": {
        "id": "e_YFWGKh-kV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Best Model\n",
        "This ensures that the version of the model that performed best on the validation data is used for predictions.\n"
      ],
      "metadata": {
        "id": "9QzUF9rY6bEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('best_model3.h5')"
      ],
      "metadata": {
        "id": "ltnjLjqfmYIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Music Predictions\n",
        "This function generates a sequence of predicted notes using the trained model. Starting with a random sequence from the validation set, it predicts the next note iteratively. The function is used to create five separate sequences, which are then flattened into a single combined sequence for further processing.\n"
      ],
      "metadata": {
        "id": "1DXRlm2b64dZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prediction(model, start_index, length=10, no_of_timesteps=32):\n",
        "    random_music = x_val[start_index]\n",
        "\n",
        "    predictions = []\n",
        "    for i in range(length):\n",
        "        random_music = random_music.reshape(1, no_of_timesteps)\n",
        "\n",
        "        prob = model.predict(random_music)[0]\n",
        "        y_pred = np.random.choice(range(len(prob)), p=prob)\n",
        "        predictions.append(y_pred)\n",
        "\n",
        "        random_music = np.insert(random_music[0], len(random_music[0]), y_pred)\n",
        "        random_music = random_music[1:]\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Generate 5 separate sequences\n",
        "sequences = []\n",
        "for _ in range(5):\n",
        "    ind = np.random.randint(0, len(x_val) - 1)\n",
        "    sequence = generate_prediction(model, ind)\n",
        "    sequences.append(sequence)\n",
        "\n",
        "# Flatten the list of lists into a single list\n",
        "combined_sequence = [note for sequence in sequences for note in sequence]\n",
        "print(combined_sequence)\n"
      ],
      "metadata": {
        "id": "wF87tLOZmYGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting Integers Back to Notes\n"
      ],
      "metadata": {
        "id": "hdNAO8dI7Oz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_int_to_note = dict((number, note_) for number, note_ in enumerate(unique_x))\n",
        "predicted_notes = [x_int_to_note[i] for i in combined_sequence]"
      ],
      "metadata": {
        "id": "iIKujZcMmYD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting Predictions to MIDI Format\n",
        "This function takes the predicted notes and converts them into a MIDI format. It addresses both single notes and chords, assigning them to a violin instrument part. This allows the playback and evaluation of the generated music.\n"
      ],
      "metadata": {
        "id": "NNNL0_AG7Uan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from music21 import note, chord, instrument, stream, pitch\n",
        "\n",
        "def convert_to_midi(prediction_output):\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # Creating a violin part\n",
        "    violin_part = stream.Part()\n",
        "    violin_part.insert(0, instrument.Violin())\n",
        "\n",
        "    for pattern in prediction_output:\n",
        "        # Check if the pattern is an integer or a float (MIDI note number)\n",
        "        if isinstance(pattern, (int, float)):\n",
        "            try:\n",
        "                # Convert float to int for MIDI note number\n",
        "                pattern = int(pattern)\n",
        "                new_note = note.Note(pattern)\n",
        "                new_note.storedInstrument = instrument.Violin()\n",
        "                new_note.offset = offset\n",
        "                output_notes.append(new_note)\n",
        "            except pitch.PitchException as e:\n",
        "                print(f\"Invalid MIDI note number: {pattern}, Error: {e}\")\n",
        "            offset += 0.5\n",
        "            continue\n",
        "\n",
        "        # Check if pattern is a string representing chords\n",
        "        elif isinstance(pattern, str) and ('.' in pattern or pattern.isdigit()):\n",
        "            notes_in_chord = pattern.split('.')\n",
        "            notes = []\n",
        "            for current_note in notes_in_chord:\n",
        "                try:\n",
        "                    new_note = note.Note(int(current_note))\n",
        "                    new_note.storedInstrument = instrument.Violin()\n",
        "                    notes.append(new_note)\n",
        "                except pitch.PitchException as e:\n",
        "                    print(f\"Invalid note pattern in chord: {current_note}, Error: {e}\")\n",
        "                    continue\n",
        "\n",
        "            if notes:\n",
        "                new_chord = chord.Chord(notes)\n",
        "                new_chord.offset = offset\n",
        "                output_notes.append(new_chord)\n",
        "\n",
        "        offset += 0.5\n",
        "\n",
        "    # Add notes to the violin part\n",
        "    for n in output_notes:\n",
        "        violin_part.append(n)\n",
        "\n",
        "    # Create a MIDI stream and write the file\n",
        "    midi_stream = stream.Stream()\n",
        "    midi_stream.append(violin_part)\n",
        "    midi_stream.write('midi', fp='music.mid')\n",
        "\n",
        "convert_to_midi(predicted_notes)\n"
      ],
      "metadata": {
        "id": "bW35_lSN4gap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second Iteration of WaveNet Model Training\n",
        "\n",
        "### Goals\n",
        "- Address overfitting observed in the first model iteration.\n",
        "- Improve model generalization on validation data.\n",
        "\n",
        "### Updates\n",
        "1. **Model Architecture**: Add regularization, modify layer configurations.\n",
        "2. **Optimization**: Adjust learning rate, implement early stopping.\n",
        "3. **Data Handling**: Enhance data augmentation, revise feature engineering.\n",
        "4. **Training Approach**: Modifiy batch size and epoch strategy.\n",
        "5. **Hyperparameter Tuning**: Conduct systematic hyperparameter optimization."
      ],
      "metadata": {
        "id": "cYI1TQhmFMpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Updated Model Architecture"
      ],
      "metadata": {
        "id": "zlmIaNegG_dT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding, Conv1D, Dropout, MaxPool1D, GlobalMaxPool1D, Dense\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import keras.backend as K\n",
        "\n",
        "# Clearing previous session!\n",
        "K.clear_session()\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Embedding layer\n",
        "model.add(Embedding(len(unique_x), 100, input_length=32, trainable=True))\n",
        "\n",
        "# Convolutional Layer 1 - increased dropout\n",
        "model.add(Conv1D(64, 3, padding='causal', activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(MaxPool1D(2))\n",
        "\n",
        "# Convolutional Layer 2 - consistent dropout to regularize\n",
        "model.add(Conv1D(128, 3, activation='relu', dilation_rate=2, padding='causal'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(MaxPool1D(2))\n",
        "\n",
        "# Convolutional Layer 3 - regularize\n",
        "model.add(Conv1D(256, 3, activation='relu', dilation_rate=4, padding='causal'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(MaxPool1D(2))\n",
        "\n",
        "# Global Max Pooling\n",
        "model.add(GlobalMaxPool1D())\n",
        "\n",
        "# Dense Layer - additional dropout layer\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(len(unique_y), activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "Rp7HhsD4FtFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Callbacks"
      ],
      "metadata": {
        "id": "8-SWA-3SGbtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')\n",
        "model_checkpoint = ModelCheckpoint('best_model_updated.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)"
      ],
      "metadata": {
        "id": "9K1ING_tGLNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Augmentation"
      ],
      "metadata": {
        "id": "NfouPZ7cHw7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_sequence(sequence, max_shift=3):\n",
        "    # Shift the sequence by a small integer amount for augmentation\n",
        "    shift = random.randint(-max_shift, max_shift)\n",
        "    augmented_sequence = sequence + shift\n",
        "    augmented_sequence = np.clip(augmented_sequence, 0, len(unique_x) - 1)\n",
        "    return augmented_sequence\n",
        "\n",
        "# Augmenting the training data\n",
        "x_tr_augmented = [augment_sequence(seq) for seq in x_tr]\n",
        "y_tr_augmented = y_tr[:]\n",
        "\n",
        "# Convert augmented data to numpy arrays\n",
        "x_tr_augmented = np.array(x_tr_augmented, dtype='int32')\n",
        "\n",
        "x_val = np.array(x_val, dtype='int32')\n",
        "y_val = np.array(y_val, dtype='int32')"
      ],
      "metadata": {
        "id": "YZvFuwp9Lwf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "5VnvX5rEGdsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_tr_augmented, y_tr_augmented,\n",
        "                    batch_size=128, # Adjusted batch size\n",
        "                    epochs=100, # Incrased epochs\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    verbose=1,\n",
        "                    callbacks=[early_stopping, model_checkpoint])"
      ],
      "metadata": {
        "id": "l44Pf47AMJc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis of Second Iteration Model\n",
        "\n",
        "#### Overview\n",
        "The model's training results over 42 epochs before early stopping.\n",
        "\n",
        "#### Observations\n",
        "**Moderate Learning:** The model showed moderate learning capability with some improvements in loss and accuracy, but not significant.\n",
        "**Generalization Gap:** There is a noticeable gap between training and validation performance, suggesting issues with generalization.\n",
        "- **Best Model Consideration**: The model saved at epoch 37 is likely the best model to use because it has the lowest validation loss.\n",
        "  - Loss = 3.11492\n",
        "  - Accuracy = .5032\n",
        "\n",
        "### Conclusion\n",
        "The model has shown some learning capability, but there is significant room for improvement, especially with generalization to validation data."
      ],
      "metadata": {
        "id": "yqJanhXPN5vF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Best Model"
      ],
      "metadata": {
        "id": "e6L0__Luom0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('best_model_updated.h5')"
      ],
      "metadata": {
        "id": "EYHHr3FmTqCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Predictions & Converting to MIDI"
      ],
      "metadata": {
        "id": "1G6lPb-_XAlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prediction(model, start_index, length=10, no_of_timesteps=32):\n",
        "    random_music = x_val[start_index]\n",
        "\n",
        "    predictions = []\n",
        "    for i in range(length):\n",
        "        random_music = random_music.reshape(1, no_of_timesteps)\n",
        "\n",
        "        prob = model.predict(random_music)[0]\n",
        "        y_pred = np.random.choice(range(len(prob)), p=prob)\n",
        "        predictions.append(y_pred)\n",
        "\n",
        "        random_music = np.insert(random_music[0], len(random_music[0]), y_pred)\n",
        "        random_music = random_music[1:]\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Generate 5 separate sequences\n",
        "sequences = []\n",
        "for _ in range(5):\n",
        "    ind = np.random.randint(0, len(x_val) - 1)\n",
        "    sequence = generate_prediction(model, ind)\n",
        "    sequences.append(sequence)\n",
        "\n",
        "# Flatten the list of lists into a single list\n",
        "combined_sequence_2 = [note for sequence in sequences for note in sequence]\n",
        "#print(combined_sequence_2)\n"
      ],
      "metadata": {
        "id": "N9Ta6tH_WGwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_int_to_note = dict((number, note_) for number, note_ in enumerate(unique_x))\n",
        "predicted_notes_2 = [x_int_to_note[i] for i in combined_sequence_2]"
      ],
      "metadata": {
        "id": "ulrsB2mFWbOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_midi(prediction_output):\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # Creating a violin part\n",
        "    violin_part = stream.Part()\n",
        "    violin_part.insert(0, instrument.Violin())\n",
        "\n",
        "    for pattern in prediction_output:\n",
        "        # Check if the pattern is an integer or a float (MIDI note number)\n",
        "        if isinstance(pattern, (int, float)):\n",
        "            try:\n",
        "                # Convert float to int for MIDI note number\n",
        "                pattern = int(pattern)\n",
        "                new_note = note.Note(pattern)\n",
        "                new_note.storedInstrument = instrument.Violin()\n",
        "                new_note.offset = offset\n",
        "                output_notes.append(new_note)\n",
        "            except pitch.PitchException as e:\n",
        "                print(f\"Invalid MIDI note number: {pattern}, Error: {e}\")\n",
        "            offset += 0.5\n",
        "            continue\n",
        "\n",
        "        # Check if pattern is a string representing chords\n",
        "        elif isinstance(pattern, str) and ('.' in pattern or pattern.isdigit()):\n",
        "            notes_in_chord = pattern.split('.')\n",
        "            notes = []\n",
        "            for current_note in notes_in_chord:\n",
        "                try:\n",
        "                    new_note = note.Note(int(current_note))\n",
        "                    new_note.storedInstrument = instrument.Violin()\n",
        "                    notes.append(new_note)\n",
        "                except pitch.PitchException as e:\n",
        "                    print(f\"Invalid note pattern in chord: {current_note}, Error: {e}\")\n",
        "                    continue\n",
        "\n",
        "            if notes:\n",
        "                new_chord = chord.Chord(notes)\n",
        "                new_chord.offset = offset\n",
        "                output_notes.append(new_chord)\n",
        "\n",
        "        offset += 0.5\n",
        "\n",
        "    # Add notes to the violin part\n",
        "    for n in output_notes:\n",
        "        violin_part.append(n)\n",
        "\n",
        "    # Create a MIDI stream and write the file\n",
        "    midi_stream = stream.Stream()\n",
        "    midi_stream.append(violin_part)\n",
        "    midi_stream.write('midi', fp='music2.mid')\n",
        "\n",
        "convert_to_midi(predicted_notes_2)\n"
      ],
      "metadata": {
        "id": "v4XDz2w8WjMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Third Iteration of WaveNet Model"
      ],
      "metadata": {
        "id": "4IH3uxVCjfJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Goals\n",
        "- Enhance the model's ability to capture complex features and relationships in the audio data.\n",
        "- Improve the model's performance and generalization abilities, especially for generating high-quality audio waveforms.\n",
        "\n",
        "### Updates\n",
        "**Model Architecture:**\n",
        "  - Additional convolutional layers\n",
        "  - Dilation rates\n",
        "  - Residual connections\n",
        "  - Attention mechanisms\n",
        "  - Replaced ReLU with advanced activation functions like LeakyReLU for better non-linearity handling\n",
        "  - Mixed pooling\n",
        "  - Batch normalization"
      ],
      "metadata": {
        "id": "faKfwXNBj1TR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding, Conv1D, Dropout, MaxPool1D, GlobalMaxPool1D, Dense, BatchNormalization, LeakyReLU, Flatten\n",
        "from keras.models import Sequential\n",
        "import keras.backend as K\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Embedding layer\n",
        "model.add(Embedding(len(unique_x), 100, input_length=32, trainable=True))\n",
        "\n",
        "# Reshape if necessary\n",
        "model.add(Reshape((32, 100)))\n",
        "\n",
        "# Increasing depth and complexity of the model\n",
        "for i in range(4):\n",
        "    model.add(Conv1D(64 * 2**i, 3, dilation_rate=2**i, padding='causal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(Dropout(0.3))\n",
        "    if i < 3:  # Adding pooling except for the last convolutional block\n",
        "        model.add(MaxPool1D(2))\n",
        "\n",
        "# Global Max Pooling\n",
        "model.add(GlobalMaxPool1D())\n",
        "\n",
        "# Dense Layers\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(len(unique_y), activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "F4b0k4MRjcPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Callbacks"
      ],
      "metadata": {
        "id": "mkzpA7bgnvdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')\n",
        "model_checkpoint = ModelCheckpoint('best_model_updated2.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)"
      ],
      "metadata": {
        "id": "4od5Ky6sn3Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_tr_augmented, y_tr_augmented,\n",
        "                    batch_size=128, # Adjusted batch size\n",
        "                    epochs=100, # Incrased epochs\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    verbose=1,\n",
        "                    callbacks=[early_stopping, model_checkpoint])"
      ],
      "metadata": {
        "id": "yNrSjNIwoTYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Best Model"
      ],
      "metadata": {
        "id": "gOv2C8OWorZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('best_model_updated2.h5')"
      ],
      "metadata": {
        "id": "j08veE2lotsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Predictions & Converting to MIDI"
      ],
      "metadata": {
        "id": "lAJhx_nJo5d3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prediction(model, start_index, length=10, no_of_timesteps=32):\n",
        "    random_music = x_val[start_index]\n",
        "\n",
        "    predictions = []\n",
        "    for i in range(length):\n",
        "        random_music = random_music.reshape(1, no_of_timesteps)\n",
        "\n",
        "        prob = model.predict(random_music)[0]\n",
        "        y_pred = np.random.choice(range(len(prob)), p=prob)\n",
        "        predictions.append(y_pred)\n",
        "\n",
        "        random_music = np.insert(random_music[0], len(random_music[0]), y_pred)\n",
        "        random_music = random_music[1:]\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Generate 5 separate sequences\n",
        "sequences = []\n",
        "for _ in range(5):\n",
        "    ind = np.random.randint(0, len(x_val) - 1)\n",
        "    sequence = generate_prediction(model, ind)\n",
        "    sequences.append(sequence)\n",
        "\n",
        "# Flatten the list of lists into a single list\n",
        "combined_sequence_3 = [note for sequence in sequences for note in sequence]\n",
        "print(combined_sequence_3)\n"
      ],
      "metadata": {
        "id": "mzZ6018vpCxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_int_to_note = dict((number, note_) for number, note_ in enumerate(unique_x))\n",
        "predicted_notes_3 = [x_int_to_note[i] for i in combined_sequence_3]"
      ],
      "metadata": {
        "id": "lgYckqnhpMP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_midi(prediction_output):\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # Creating a violin part\n",
        "    violin_part = stream.Part()\n",
        "    violin_part.insert(0, instrument.Violin())\n",
        "\n",
        "    for pattern in prediction_output:\n",
        "        # Check if the pattern is an integer or a float (MIDI note number)\n",
        "        if isinstance(pattern, (int, float)):\n",
        "            try:\n",
        "                # Convert float to int for MIDI note number\n",
        "                pattern = int(pattern)\n",
        "                new_note = note.Note(pattern)\n",
        "                new_note.storedInstrument = instrument.Violin()\n",
        "                new_note.offset = offset\n",
        "                output_notes.append(new_note)\n",
        "            except pitch.PitchException as e:\n",
        "                print(f\"Invalid MIDI note number: {pattern}, Error: {e}\")\n",
        "            offset += 0.5\n",
        "            continue\n",
        "\n",
        "        # Check if pattern is a string representing chords\n",
        "        elif isinstance(pattern, str) and ('.' in pattern or pattern.isdigit()):\n",
        "            notes_in_chord = pattern.split('.')\n",
        "            notes = []\n",
        "            for current_note in notes_in_chord:\n",
        "                try:\n",
        "                    new_note = note.Note(int(current_note))\n",
        "                    new_note.storedInstrument = instrument.Violin()\n",
        "                    notes.append(new_note)\n",
        "                except pitch.PitchException as e:\n",
        "                    print(f\"Invalid note pattern in chord: {current_note}, Error: {e}\")\n",
        "                    continue\n",
        "\n",
        "            if notes:\n",
        "                new_chord = chord.Chord(notes)\n",
        "                new_chord.offset = offset\n",
        "                output_notes.append(new_chord)\n",
        "\n",
        "        offset += 0.5\n",
        "\n",
        "    # Add notes to the violin part\n",
        "    for n in output_notes:\n",
        "        violin_part.append(n)\n",
        "\n",
        "    # Create a MIDI stream and write the file\n",
        "    midi_stream = stream.Stream()\n",
        "    midi_stream.append(violin_part)\n",
        "    midi_stream.write('midi', fp='music3.mid')\n",
        "\n",
        "convert_to_midi(predicted_notes_3)\n"
      ],
      "metadata": {
        "id": "deaUFOFQpWoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MIDI to WAV Conversion using FluidSynth\n",
        "\n",
        "I will utilize the `FluidSynth` library to convert MIDI files into WAV format in order to proceed with further evaluation metrics.\n"
      ],
      "metadata": {
        "id": "g1UtNvpeOUmC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qf6O6Wsh_3s"
      },
      "outputs": [],
      "source": [
        "!apt install -y fluidsynth\n",
        "!pip install midi2audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoAkgDnvieiU"
      },
      "outputs": [],
      "source": [
        "from midi2audio import FluidSynth\n",
        "\n",
        "# Initialize FluidSynth with a sound font\n",
        "fs = FluidSynth('/content/gdrive/MyDrive/Violin_Comp_Data/soundfonts/Acro_Violins.sf2')\n",
        "\n",
        "# Convert MIDI to WAV\n",
        "fs.midi_to_audio('music.mid', 'wavenet1_acro.wav')\n",
        "fs.midi_to_audio('music2.mid', 'wavenet2_round2_acro.wav')\n",
        "fs.midi_to_audio('music3.mid', 'wavenet3_round7_acro.wav')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Music Generation: Pitch and Rhythm Consistency\n",
        "Together, pitch and rhythm consistency form two fundamental pillars of music that determine its overall quality and appeal. By evaluating these aspects, we can gauge the success of our music generation models in producing compositions that are not just technically sound but also musically coherent and enjoyable.\n",
        "\n"
      ],
      "metadata": {
        "id": "p1sz7v9I4N9q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_86mJgzFPGBO"
      },
      "source": [
        "### Pitch Consistency\n",
        "Pitch consistency can be evaluated by extracting the pitch from the audio and then analyzing its stability and variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xB9ZaSxPJy6"
      },
      "outputs": [],
      "source": [
        "def calculate_pitch_consistency(audio, sr):\n",
        "    # Extract pitch\n",
        "    pitches, magnitudes = librosa.piptrack(y=audio, sr=sr)\n",
        "    # Select the predominant pitch at each frame\n",
        "    predominant_pitches = [pitches[magnitudes[:, t].argmax(), t] for t in range(pitches.shape[1])]\n",
        "    predominant_pitches = np.array(predominant_pitches)\n",
        "\n",
        "    # Calculate variance\n",
        "    pitch_variance = np.var(predominant_pitches)\n",
        "    return pitch_variance\n",
        "\n",
        "# Baseline WaveNet\n",
        "audio, sr = librosa.load('wavenet1_acro.wav')\n",
        "pitch_variance1 = calculate_pitch_consistency(audio, sr)\n",
        "print(\"Baseline WaveNet Pitch Variance:\", pitch_variance1)\n",
        "\n",
        "# First iteration WaveNet\n",
        "audio, sr = librosa.load('wavenet2_round2_acro.wav')\n",
        "pitch_variance2 = calculate_pitch_consistency(audio, sr)\n",
        "print(\"1st Iteration WaveNet Pitch Variance:\", pitch_variance2)\n",
        "\n",
        "# Second iteration WaveNet\n",
        "audio, sr = librosa.load('wavenet3_round7_acro.wav')\n",
        "pitch_variance3 = calculate_pitch_consistency(audio, sr)\n",
        "print(\"2nd Iteration WaveNet Pitch Variance:\", pitch_variance3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO7oaEcUPM32"
      },
      "source": [
        "### Rhythm Consistency\n",
        "Rhythm consistency can be evaluated by analyzing the beat and tempo of the generated audio.\n",
        "- Extract Beat Information\n",
        "- Analyze Tempo Stability Over Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14BzmPGjPREg"
      },
      "outputs": [],
      "source": [
        "def calculate_rhythm_consistency(file_path):\n",
        "    audio, sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "    # Track beats\n",
        "    tempo, beats = librosa.beat.beat_track(y=audio, sr=sr)\n",
        "    beat_times = librosa.frames_to_time(beats, sr=sr)\n",
        "\n",
        "    # Calculate tempo variability\n",
        "    inter_beat_intervals = np.diff(beat_times)\n",
        "    tempo_variability = np.std(inter_beat_intervals)\n",
        "\n",
        "    return tempo, tempo_variability\n",
        "\n",
        "file_paths = ['wavenet1_acro.wav',\n",
        "              'wavenet2_round2_acro.wav',\n",
        "              'wavenet3_round7_acro.wav']\n",
        "\n",
        "# Calculate and display rhythm consistency for each file\n",
        "for i, file_path in enumerate(file_paths):\n",
        "    tempo, tempo_variability = calculate_rhythm_consistency(file_path)\n",
        "    print(f\"File {i + 1}:\")\n",
        "    print(f\"Path: {file_path}\")\n",
        "    print(f\"Tempo: {tempo}\")\n",
        "    print(f\"Tempo Variability: {tempo_variability}\")\n",
        "    print(\"------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis of Pitch and Rhythm Consistency Results\n",
        "The WaveNet models are showing promising results in rhythm generation, maintaining consistent tempos and rhythmic structures. However, the increasing pitch variance suggests a need to refine the model to improve pitch consistency and thereby enhance the overall cohesiveness and quality of the generated music.\n",
        "\n",
        "## Conclusion\n",
        "From here, I will implement LSTM models, which are tailored to understand and replicate the complexities of musical compositions."
      ],
      "metadata": {
        "id": "Dxt0nvgP60D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proceed to 'Model_2_LSTM.ipynb'\n",
        "``` bash\n",
        "├── AI_Violinist_Intro.ipynb                <- Data capture/project overview\n",
        "├── Model_1_WaveNet.ipynb                   <- Baseline/WaveNet Models\n",
        "├── Model_2_LSTM.ipynb                      <- First LSTM Model\n",
        "├── Model_3__Complex_LSTM.ipynb             <- Second LSTM Model\n",
        "├── Visual_Analysis_Model_Comparison.ipynb  <- Model Evaluation\n",
        "├── Pretrained_Model_Jukebox.ipynb          <- Generating Final Music\n",
        "└── Failed_Models_Spectrograms.ipynb        <- Failed attempts\n",
        "```\n"
      ],
      "metadata": {
        "id": "qoZ5baL28WkO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J4kbF1am8X6v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
