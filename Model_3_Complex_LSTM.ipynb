{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "_-tkIHednLFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enhanced LSTM Model for Music Generation\n",
        "Building upon the foundations of recurrent neural networks, this model leverages the power of bidirectional LSTMs and regularization techniques to create a more robust and capable system for music generation tasks.\n",
        "\n",
        "## Enhanced Sampling for Generation\n",
        "Implementing Temperature Sampling will allow control of the randomness of predictions. A higher temperature results in more random outputs, and a lower temperature makes the model's outputs more deterministic.\n"
      ],
      "metadata": {
        "id": "7QyVpT5v-rGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation"
      ],
      "metadata": {
        "id": "s7OiSdrmRt4D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-ATcHWKkMve"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from music21 import converter, instrument, note, chord\n",
        "\n",
        "def read_midi(file):\n",
        "    print(\"Loading Music File:\", file)\n",
        "    notes = []\n",
        "\n",
        "    midi = converter.parse(file)\n",
        "    parts = instrument.partitionByInstrument(midi)\n",
        "    relevant_parts = parts.parts if parts else [midi]\n",
        "\n",
        "    for part in relevant_parts:\n",
        "        if 'Violin' in str(part.getInstrument()) or 'Violin' in str(part.partName):\n",
        "            for element in part.recurse():\n",
        "                if isinstance(element, note.Note):\n",
        "                    notes.append((str(element.pitch), element.duration.quarterLength, element.offset))\n",
        "                elif isinstance(element, chord.Chord):\n",
        "                    notes.append(('.'.join(str(n) for n in element.normalOrder), element.duration.quarterLength, element.offset))\n",
        "                elif isinstance(element, note.Rest):\n",
        "                    notes.append(('rest', element.duration.quarterLength, element.offset))\n",
        "\n",
        "    return notes\n",
        "\n",
        "\n",
        "path = '/content/gdrive/MyDrive/Violin_Comp_Data/midi_150/'\n",
        "files = [i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
        "notes_array = [read_midi(os.path.join(path, file)) for file in files]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding each unique note to an integer."
      ],
      "metadata": {
        "id": "kFSRzbE2WuO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten\n",
        "all_notes = [note for sequence in notes_array for note in sequence]"
      ],
      "metadata": {
        "id": "9Lh5q_CQN4p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fractions import Fraction\n",
        "\n",
        "all_notes = [(pitch, float(duration) if isinstance(duration, Fraction) else duration,\n",
        "              float(offset) if isinstance(offset, Fraction) else offset)\n",
        "             for pitch, duration, offset in all_notes]\n"
      ],
      "metadata": {
        "id": "c_i1YG2BOOmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notes_array = [[(pitch, float(duration) if isinstance(duration, Fraction) else duration,\n",
        "                 float(offset) if isinstance(offset, Fraction) else offset)\n",
        "                for pitch, duration, offset in sequence]\n",
        "               for sequence in notes_array]\n"
      ],
      "metadata": {
        "id": "YLBsO1HBORKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "note_to_int = {note: i for i, note in enumerate(sorted(set(all_notes)))}\n"
      ],
      "metadata": {
        "id": "3n7HKWl1OT10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "output_notes = []\n",
        "no_of_timesteps = 100\n",
        "\n",
        "for notes in notes_array:\n",
        "    for i in range(len(notes) - no_of_timesteps):\n",
        "        input_seq = notes[i:i + no_of_timesteps]\n",
        "        output_note = notes[i + no_of_timesteps]\n",
        "        input_sequences.append([note_to_int[note] for note in input_seq])\n",
        "        output_notes.append(note_to_int[output_note])\n",
        "\n",
        "x_seq = np.array(input_sequences)\n",
        "y_seq = np.array(output_notes)\n"
      ],
      "metadata": {
        "id": "30rW9Qf7OWIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initiating Train-Test Split & Reshaping Input for LSTM Model"
      ],
      "metadata": {
        "id": "-FNWACczUx9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(x_seq, y_seq, test_size=0.2, random_state=13)\n",
        "x_tr = np.reshape(x_tr, (x_tr.shape[0], no_of_timesteps, 1))\n",
        "x_val = np.reshape(x_val, (x_val.shape[0], no_of_timesteps, 1))\n"
      ],
      "metadata": {
        "id": "rORuqgonXcTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adjusting LSTM Model Complexity"
      ],
      "metadata": {
        "id": "OohVKtacSrIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Model Configuration\n",
        "no_of_timesteps = 100\n",
        "num_notes = len(note_to_int)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# First Bidirectional LSTM Layer\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True, kernel_regularizer=l2(0.001)), input_shape=(no_of_timesteps, 1)))\n",
        "\n",
        "# Second LSTM Layer\n",
        "model.add(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.001)))\n",
        "\n",
        "# Third Bidirectional LSTM Layer\n",
        "model.add(Bidirectional(LSTM(64, kernel_regularizer=l2(0.001))))\n",
        "\n",
        "# Dense Layer with Regularization\n",
        "model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(num_notes, activation='softmax'))\n",
        "\n",
        "# Optimizer Configuration\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "qa49DYYvmYQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Checkpoint"
      ],
      "metadata": {
        "id": "sQ75RPLZ_Mih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "mc = ModelCheckpoint('best_model_lstm2.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)"
      ],
      "metadata": {
        "id": "sBjqEA7WmYOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Model"
      ],
      "metadata": {
        "id": "LFg8TNpb_O0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_tr, y_tr, epochs=50, batch_size=64, validation_data=(x_val, y_val), callbacks=[mc])"
      ],
      "metadata": {
        "id": "wRl6w_DFmYMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Best Model"
      ],
      "metadata": {
        "id": "8SqKZxRr_SoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('best_model_lstm2.h5')"
      ],
      "metadata": {
        "id": "ltnjLjqfmYIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Temperature Sampling"
      ],
      "metadata": {
        "id": "h58WC1Uw8gj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_with_temperature(probabilities, temperature=.96):\n",
        "    if temperature <= 0:\n",
        "        return np.argmax(probabilities)\n",
        "    else:\n",
        "        probabilities = np.asarray(probabilities).astype('float64')\n",
        "        probabilities = np.log(probabilities + 1e-7) / temperature\n",
        "        exp_probs = np.exp(probabilities)\n",
        "        probabilities = exp_probs / np.sum(exp_probs)\n",
        "        return np.random.choice(range(len(probabilities)), p=probabilities)\n"
      ],
      "metadata": {
        "id": "9zS5Kuz68YdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_music(model, start_sequence, length=50, temperature=.96, lookback_length=100):\n",
        "    prediction_output = []\n",
        "\n",
        "    # Ensuring start_sequence is of length lookback_length\n",
        "    if len(start_sequence) > lookback_length:\n",
        "        start_sequence = start_sequence[-lookback_length:]\n",
        "    elif len(start_sequence) < lookback_length:\n",
        "        # Pad the sequence\n",
        "        start_sequence = [('rest', 0, 0)] * (lookback_length - len(start_sequence)) + start_sequence\n",
        "\n",
        "    start_sequence_formatted = np.array([note_to_int[note] for note in start_sequence])\n",
        "\n",
        "    for note_index in range(length):\n",
        "        prediction_input = np.reshape(start_sequence_formatted, (1, lookback_length, 1))\n",
        "        prob = model.predict(prediction_input)[0]\n",
        "        index = sample_with_temperature(prob, temperature)\n",
        "        predicted_note = x_int_to_note[index]\n",
        "        prediction_output.append(predicted_note)\n",
        "\n",
        "        # Update start_sequence_formatted for the next prediction\n",
        "        start_sequence_formatted = np.append(start_sequence_formatted, [index])[-lookback_length:]\n",
        "\n",
        "    return prediction_output\n",
        "\n",
        "\n",
        "# Create the inverse mapping from integers back to note tuples\n",
        "x_int_to_note = dict((number, note) for note, number in note_to_int.items())\n"
      ],
      "metadata": {
        "id": "wF87tLOZmYGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert to MIDI & Generate Music"
      ],
      "metadata": {
        "id": "wg1xVteHJL4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from music21 import pitch\n",
        "\n",
        "def midi_number_to_note_name(midi_number):\n",
        "    return pitch.Pitch(midi=midi_number).nameWithOctave\n"
      ],
      "metadata": {
        "id": "MqB3wXP-QQ5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from music21 import stream, instrument, note, chord\n",
        "\n",
        "def convert_to_midi(prediction_output):\n",
        "    midi_stream = stream.Stream()\n",
        "    midi_stream.append(instrument.Violin())\n",
        "\n",
        "    offset = 0\n",
        "    for i, note_info in enumerate(prediction_output):\n",
        "        try:\n",
        "            note_name = note_info[0]\n",
        "            # Check if note_name is a MIDI number and convert it\n",
        "            if note_name.isdigit():\n",
        "                note_name = midi_number_to_note_name(int(note_name))\n",
        "\n",
        "            # Create note or rest\n",
        "            if note_name != 'rest':\n",
        "                new_note = note.Note(note_name)\n",
        "            else:\n",
        "                new_note = note.Rest()\n",
        "\n",
        "            new_note.duration.quarterLength = note_info[1]\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = instrument.Violin()\n",
        "            midi_stream.append(new_note)\n",
        "            offset += new_note.duration.quarterLength\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing note at position {i}: {note_info}. Error: {e}\")\n",
        "\n",
        "    midi_stream.write('midi', fp='lstm_music2.mid')\n",
        "\n",
        "# Randomly select a starting sequence from x_val\n",
        "random_index = np.random.randint(0, len(x_val))\n",
        "start_sequence = x_val[random_index]\n",
        "\n",
        "# Since start_sequence is currently encoded as integers, decode it back to note information\n",
        "start_sequence_decoded = [x_int_to_note[note] for note in start_sequence.flatten()]\n",
        "\n",
        "# Generate music based on the starting sequence\n",
        "prediction_output = generate_music(model, start_sequence_decoded)\n",
        "convert_to_midi(prediction_output)"
      ],
      "metadata": {
        "id": "yUEJ-01RQW0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MIDI to WAV Conversion using FluidSynth\n"
      ],
      "metadata": {
        "id": "g1UtNvpeOUmC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qf6O6Wsh_3s"
      },
      "outputs": [],
      "source": [
        "!apt install -y fluidsynth\n",
        "!pip install midi2audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoAkgDnvieiU"
      },
      "outputs": [],
      "source": [
        "from midi2audio import FluidSynth\n",
        "\n",
        "# Initialize FluidSynth with a sound font\n",
        "fs = FluidSynth('/content/gdrive/MyDrive/Violin_Comp_Data/soundfonts/Acro_Violins.sf2')\n",
        "\n",
        "# Convert MIDI to WAV\n",
        "fs.midi_to_audio('lstm_music2.mid', 'lstm2_acro.wav')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Music Generation: Pitch and Rhythm Consistency\n",
        "Together, pitch and rhythm consistency form two fundamental pillars of music that determine its overall quality and appeal. By evaluating these aspects, we can gauge the success of our music generation models in producing compositions that are not just technically sound but also musically coherent and enjoyable.\n",
        "\n"
      ],
      "metadata": {
        "id": "p1sz7v9I4N9q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_86mJgzFPGBO"
      },
      "source": [
        "### Pitch Consistency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xB9ZaSxPJy6"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "\n",
        "def calculate_pitch_consistency(audio, sr):\n",
        "    # Extract pitch\n",
        "    pitches, magnitudes = librosa.piptrack(y=audio, sr=sr)\n",
        "    # Select the predominant pitch at each frame\n",
        "    predominant_pitches = [pitches[magnitudes[:, t].argmax(), t] for t in range(pitches.shape[1])]\n",
        "    predominant_pitches = np.array(predominant_pitches)\n",
        "\n",
        "    # Calculate variance\n",
        "    pitch_variance = np.var(predominant_pitches)\n",
        "    return pitch_variance\n",
        "\n",
        "# Baseline WaveNet\n",
        "audio, sr = librosa.load('lstm2_acro.wav')\n",
        "pitch_variance = calculate_pitch_consistency(audio, sr)\n",
        "print(\"Second Iteration LSTM Pitch Variance:\", pitch_variance)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO7oaEcUPM32"
      },
      "source": [
        "### Rhythm Consistency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14BzmPGjPREg"
      },
      "outputs": [],
      "source": [
        "def calculate_rhythm_consistency(file_path):\n",
        "    audio, sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "    # Track beats\n",
        "    tempo, beats = librosa.beat.beat_track(y=audio, sr=sr)\n",
        "    beat_times = librosa.frames_to_time(beats, sr=sr)\n",
        "\n",
        "    # Calculate tempo variability\n",
        "    inter_beat_intervals = np.diff(beat_times)\n",
        "    tempo_variability = np.std(inter_beat_intervals)\n",
        "\n",
        "    return tempo, tempo_variability\n",
        "\n",
        "file_paths = ['lstm2_acro.wav']\n",
        "\n",
        "# Calculate and display rhythm consistency for each file\n",
        "for i, file_path in enumerate(file_paths):\n",
        "    tempo, tempo_variability = calculate_rhythm_consistency(file_path)\n",
        "    print(f\"File {i + 1}:\")\n",
        "    print(f\"Path: {file_path}\")\n",
        "    print(f\"Tempo: {tempo}\")\n",
        "    print(f\"Tempo Variability: {tempo_variability}\")\n",
        "    print(\"------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis of Pitch and Rhythm Consistency Results\n",
        "**Pitch Consistency:** The pitch variance in the Second Iteration LSTM model is 33,523.45, which is an improvement compared to the first iteration's 42,279.527. This decrease suggests that the model is getting better at maintaining a consistent pitch pattern, leading to potentially more coherent and harmonious music. However, the variance is still relatively high, indicating room for further improvement in capturing and generating stable pitch patterns.\n",
        "\n",
        "**Rhythm Consistency:** The tempo increased in variability from the first LSTM model, indicating that the rhythm's timing may fluctuate more in this iteration.\n",
        "\n",
        "## Conclusion\n",
        "The second iteration LSTM model shows a promising improvement in pitch consistency but indicates a need to revisit rhythm stability. Because I suspect issues with the limited training data, I will use a pre-trained model with my input audio for the next iteration. But first, I will perform comparative model analysis to determine the best generated audio to use for the pre-trained model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pxTGJ3nFFQ3m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proceed to 'Visual_Analysis_Model_Comparison.ipynb'\n",
        "\n",
        "``` bash\n",
        "├── AI_Violinist_Intro.ipynb                <- Data capture/project overview\n",
        "├── Model_1_WaveNet.ipynb                   <- Baseline/WaveNet Models\n",
        "├── Model_2_LSTM.ipynb                      <- First LSTM Model\n",
        "├── Model_3__Complex_LSTM.ipynb             <- Second LSTM Model\n",
        "├── Visual_Analysis_Model_Comparison.ipynb  <- Model Evaluation\n",
        "├── Pretrained_Model_Jukebox.ipynb          <- Generating Final Music\n",
        "└── Failed_Models_Spectrograms.ipynb        <- Failed attempts\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "qoZ5baL28WkO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4uEQDUMEMR9o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}