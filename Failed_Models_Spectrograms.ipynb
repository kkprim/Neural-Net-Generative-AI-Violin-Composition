{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwW81kt1YuNv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Harmonizing with Failure\n",
        "This notebook documents my journey through various models and methods that did not yield the expected results when attempting to generate violin music. It's a tale of trial and error, where each misstep is a note in the learning melody.\n",
        "\n",
        "## Embracing Missteps\n",
        "- **Learning Opportunities:** Think of each failed model as a complex chord, teaching the nuances of what harmonizes well and what creates dissonance.\n",
        "\n",
        "- **Iterative Improvement:** The journey is a crescendo of iterative learning, where each attempt tunes its approach more finely.\n",
        "\n",
        "- **Insight Sharing:** Sharing my out-of-tune attempts can be informative for others in the community.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MjUncZ9KE5WZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "SQMFIDzuzlF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "import os\n",
        "import time\n",
        "from pydub import AudioSegment\n",
        "import torch\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import shutil"
      ],
      "metadata": {
        "id": "9pJtYWs-zkhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqGVhrN4stvS"
      },
      "source": [
        "## Baseline Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9CEGN5QQBBa"
      },
      "source": [
        "### Standardize Spectrogram Sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hreVDfMFP_8A"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def standardize_spectrogram_size(file_path, target_size=(128, 128)):\n",
        "    \"\"\"Resize or pad the spectrogram to the target size.\"\"\"\n",
        "    image = Image.open(file_path)\n",
        "    image = image.resize(target_size, Image.ANTIALIAS)\n",
        "    return np.array(image)\n",
        "\n",
        "folder_path = '/content/gdrive/MyDrive/Violin_Comp_Data/spectrograms'\n",
        "standardized_spectrograms = []\n",
        "\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.png'):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        spec = standardize_spectrogram_size(file_path)\n",
        "        standardized_spectrograms.append(spec)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNVoSnjwQwXA"
      },
      "source": [
        "### Scale the Spectrogram Values\n",
        "Scale the values of the spectrograms to a consistent range of [0, 1]. This step is crucial for the model to process the input effectively, and will ensure no data leakage as no statistical calculations are being made on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnu8obgpQ1qt"
      },
      "outputs": [],
      "source": [
        "scaled_spectrograms = [spec / 255.0 for spec in standardized_spectrograms]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn03pndRRd0D"
      },
      "source": [
        "### Split Spectrograms into Sequences for Generation\n",
        "I'll create overlapping input-output pairs where each input sequence is followed by its immediate next sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swfpZMmLRdVC"
      },
      "outputs": [],
      "source": [
        "sequence_length = 50\n",
        "\n",
        "def create_sequences(spectrogram, sequence_length):\n",
        "    input_sequences = []\n",
        "    output_sequences = []\n",
        "    for i in range(0, spectrogram.shape[1] - sequence_length * 2, sequence_length):\n",
        "        input_sequences.append(spectrogram[:, i:i + sequence_length])\n",
        "        output_sequences.append(spectrogram[:, i + sequence_length:i + 2 * sequence_length])\n",
        "    return input_sequences, output_sequences\n",
        "\n",
        "X, y = [], []\n",
        "for spec in scaled_spectrograms:\n",
        "    inputs, outputs = create_sequences(spec, sequence_length)\n",
        "    X.extend(inputs)\n",
        "    y.extend(outputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG22hv5gRyw3"
      },
      "source": [
        "### Perform a Train-Test Split\n",
        "Splitting sequences into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Fr-BF6FRyGP"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming X and y are lists or NumPy arrays\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYNEFpwPR-1W"
      },
      "source": [
        "### Convert the Split Data into Tensors for Training\n",
        "Convert data into tensors to prepare to use TensorFlow deep learning.\n",
        "\n",
        "- Flatten the last two dimensions. Since LSTMs don't process 2D data directly, I'll need to flatten the width and channels dimensions into a single feature dimension.\n",
        "\n",
        "- Reshape the tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWcrw9FneNEo"
      },
      "outputs": [],
      "source": [
        "print(\"X_train_tensors shape:\", X_train_tensors.shape)\n",
        "print(\"X_test_tensors shape:\", X_test_tensors.shape)\n",
        "print(\"y_train_tensors shape:\", y_train_tensors.shape)\n",
        "print(\"y_test_tensors shape:\", y_test_tensors.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4vYJeKOVEok"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def reshape_tensors(tensor):\n",
        "    batch_size, height, width, channels = tensor.shape\n",
        "    return tf.reshape(tensor, (batch_size, height, width * channels))\n",
        "\n",
        "X_train_tensors = reshape_tensors(X_train_tensors)\n",
        "X_test_tensors = reshape_tensors(X_test_tensors)\n",
        "y_train_tensors = reshape_tensors(y_train_tensors)\n",
        "y_test_tensors = reshape_tensors(y_test_tensors)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO0DrwQYcNsv"
      },
      "source": [
        "### Building the LSTM Model\n",
        "Starting with a simple one-layer LSTM for baseline.\n",
        "\n",
        "First, I'll need to reshape the data to be [batch_size, timesteps, features]. Here, timesteps could correspond to height (or width, depending on how the spectrograms are oriented), and features would be the flattened width * channels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Vt48Z0ecNG9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Activation\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X_train_tensors.shape[1], X_train_tensors.shape[2]), return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(y_train_tensors.shape[2])))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjDhVRvhckfK"
      },
      "source": [
        "### Training the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5G_GblLckG7"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train_tensors, y_train_tensors, epochs=50, batch_size=64, validation_data=(X_test_tensors, y_test_tensors))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SV0ullXhMyy"
      },
      "source": [
        "### Interpretation\n",
        "The model shows promising signs of effective learning, as evidenced by the decreasing trend in both training and validation losses. A positive aspect is the closeness of validation loss to training loss, indicating a low risk of overfitting. However, the real measure of success lies in the listening to the generated music, despite the loss values being around 0.075 at epoch 50, which seems reasonable for scaled spectrogram data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA0YKMBnc5FL"
      },
      "source": [
        "### Generating Music\n",
        "\n",
        "- Start with a seed sequence (part of the spectrogram).\n",
        "\n",
        "- Predict the next part of the spectrogram.\n",
        "\n",
        "- Append the prediction to the sequence and use it as the new seed.\n",
        "\n",
        "- Repeat the process to generate subsequent parts of the spectrogram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FxUwYuAc-Po"
      },
      "outputs": [],
      "source": [
        "def generate_music(model, seed, length=1000):\n",
        "    generated = [seed]\n",
        "    for i in range(length):\n",
        "        last_sequence = generated[-1]\n",
        "        last_sequence_reshaped = tf.reshape(last_sequence, (1, last_sequence.shape[0], last_sequence.shape[1]))\n",
        "        prediction = model.predict(last_sequence_reshaped)\n",
        "        generated.append(prediction[0])\n",
        "\n",
        "    return np.array(generated)\n",
        "\n",
        "seed = X_test_tensors[13]\n",
        "generated_music = generate_music(model, seed, length=1000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJNjvgD2iVSt"
      },
      "source": [
        "### Reconstruct the Continuous Spectrogram\n",
        "Since the generated output is a sequence of overlapping spectrogram slices, I'll need to merge these slices back into a single, continuous spectrogram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olLuCNo0ict4"
      },
      "outputs": [],
      "source": [
        "def reconstruct_spectrogram(generated_sequences):\n",
        "    # Assuming the first half of each slice overlaps with the last half of the previous slice\n",
        "    half = generated_sequences.shape[2] // 2\n",
        "    spectrogram = generated_sequences[0, :, :half]\n",
        "\n",
        "    for i in range(1, generated_sequences.shape[0]):\n",
        "        spectrogram = np.hstack((spectrogram, generated_sequences[i, :, half:]))\n",
        "\n",
        "    return spectrogram\n",
        "\n",
        "continuous_spectrogram = reconstruct_spectrogram(generated_music)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRUwPYziigz2"
      },
      "source": [
        "### Convert Spectrogram to Audio\n",
        "I'll attempt to use the Griffin-Lim algorithm to approximate the phase and invert the spectrogram back to a waveform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sD7ppFSlERL"
      },
      "outputs": [],
      "source": [
        "!pip install soundfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2Pyz0O7inVg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "output_folder = '/content/gdrive/MyDrive/Violin_Comp_Data/generated_music'\n",
        "\n",
        "# Create the output directory if it does not exist\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# Assuming 'generated_music' is a list of continuous spectrograms\n",
        "for i, continuous_spectrogram in enumerate(generated_music):\n",
        "    # Convert to amplitude\n",
        "    S = librosa.db_to_amplitude(continuous_spectrogram)\n",
        "\n",
        "    # Use Griffin-Lim to approximate the phase\n",
        "    y = librosa.griffinlim(S)\n",
        "\n",
        "    # Construct the file path\n",
        "    output_file = os.path.join(output_folder, f'generated_music_{i}.wav')\n",
        "\n",
        "    # Save the audio file using soundfile\n",
        "    sf.write(output_file, y, samplerate=22050)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPez5IAEhxhy"
      },
      "source": [
        "### Music Evaluation\n",
        "\n",
        "Unfortunately, the generated audio was a bunch of white noise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GC7iJfvskxC"
      },
      "source": [
        "## First Tuned Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5awbu7Istc3D"
      },
      "source": [
        "### Use Different Types of Spectrograms\n",
        "I'll experiment with different types of spectrograms such as Mel-spectrograms and CQT spectrograms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyNFbrsPs5Sz"
      },
      "source": [
        "### Adjust Spectrogram Generation and Preprocessing\n",
        "Change FFT Window Size\n",
        "Adjusting the FFT (Fast Fourier Transform) window size affects the resolution of the spectrogram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmQn-itZs5sX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the directory containing the audio files\n",
        "audio_files_directory = '/content/gdrive/MyDrive/Violin_Comp_Data/converted_music_files_cleaned'\n",
        "\n",
        "# Define the directory to save the spectrograms\n",
        "spectrogram_output_directory = '/content/gdrive/MyDrive/Violin_Comp_Data/spectrograms_FFT_700'\n",
        "\n",
        "# Create the spectrogram directory if it does not exist\n",
        "if not os.path.exists(spectrogram_output_directory):\n",
        "    os.makedirs(spectrogram_output_directory)\n",
        "\n",
        "# Initialize a counter for the number of processed files\n",
        "processed_files = 0\n",
        "\n",
        "# Loop through each file in the directory\n",
        "for filename in os.listdir(audio_files_directory):\n",
        "    if filename.lower().endswith('.wav'):\n",
        "        audio_file_path = os.path.join(audio_files_directory, filename)\n",
        "\n",
        "        # Load the audio file\n",
        "        y, sr = librosa.load(audio_file_path)\n",
        "\n",
        "        # Generate the STFT with a different FFT window size\n",
        "        D = librosa.stft(y, n_fft=2048)\n",
        "\n",
        "        # Convert to dB scale for visualization\n",
        "        D_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
        "\n",
        "        # Plot the spectrogram\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        librosa.display.specshow(D_db, sr=sr, x_axis='time', y_axis='log')\n",
        "        plt.colorbar(format='%+2.0f dB')\n",
        "        plt.title(f'STFT with Modified FFT Window Size - {filename}')\n",
        "\n",
        "        # Save the figure to the output folder\n",
        "        output_filename = os.path.join(spectrogram_output_directory, f'{os.path.splitext(filename)[0]}_spectrogram.png')\n",
        "        plt.savefig(output_filename)\n",
        "        plt.close()  # Close the plot to free up memory\n",
        "\n",
        "        # Increment the processed files counter\n",
        "        processed_files += 1\n",
        "        if processed_files >= 700:\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65diiGOotixs"
      },
      "source": [
        "Mel-Spectrogram:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSqypA6Mteuo"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "\n",
        "# Generate a Mel-spectrogram\n",
        "S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "S_db = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='mel')\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.title('Mel-Spectrogram')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTgDowiUtnan"
      },
      "source": [
        "Constant-Q Transform (CQT) Spectrogram:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsSx-1GOtpuB"
      },
      "outputs": [],
      "source": [
        "# Generate a CQT spectrogram\n",
        "C = librosa.cqt(y, sr=sr)\n",
        "C_db = librosa.amplitude_to_db(np.abs(C), ref=np.max)\n",
        "\n",
        "librosa.display.specshow(C_db, sr=sr, x_axis='time', y_axis='cqt_note')\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.title('CQT Spectrogram')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJa_8rMJP-Hu"
      },
      "source": [
        "### Next Steps\n",
        "\n",
        "- Load the Generated Spectrograms\n",
        "\n",
        "- Convert the Images to Arrays\n",
        "\n",
        "- Split Each Spectrogram into Input-Output Pairs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-DBtxqCP5av"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Define the directory containing the spectrogram images\n",
        "spectrogram_images_directory = '/content/gdrive/MyDrive/Violin_Comp_Data/spectrograms_FFT_700'\n",
        "\n",
        "# Parameters for splitting into sequences\n",
        "sequence_length = 50\n",
        "overlap = 25\n",
        "\n",
        "X = []  # Input sequences\n",
        "y = []  # Output sequences\n",
        "\n",
        "# Loop through each spectrogram image in the directory\n",
        "for filename in os.listdir(spectrogram_images_directory):\n",
        "    if filename.endswith('.png'):  # Check for PNG files\n",
        "        spectrogram_image_path = os.path.join(spectrogram_images_directory, filename)\n",
        "\n",
        "        # Load the spectrogram image and convert it to a NumPy array\n",
        "        image = Image.open(spectrogram_image_path).convert('L')  # Convert to grayscale\n",
        "        spectrogram = np.array(image)\n",
        "\n",
        "        # Split the spectrogram into sequences\n",
        "        for start_idx in range(0, spectrogram.shape[1] - sequence_length, sequence_length - overlap):\n",
        "            end_idx = start_idx + sequence_length\n",
        "            X.append(spectrogram[:, start_idx:end_idx])\n",
        "            y.append(spectrogram[:, start_idx + overlap:end_idx + overlap])\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wt0ArmlMWpbA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V59Zugr7RA94"
      },
      "source": [
        "### Convert to Tensors:\n",
        "After splitting the spectrograms into sequences, convert X and y to TensorFlow tensors for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QK9AhNU2Q_Wn"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "X_tensors = tf.convert_to_tensor(X, dtype=tf.float32)\n",
        "y_tensors = tf.convert_to_tensor(y, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71MHqqwLS3nM"
      },
      "source": [
        "### Convert Tensors to NumPy Arrays\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwchKAuaSvYz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "X_numpy = X_tensors.numpy()\n",
        "y_numpy = y_tensors.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpMda1tNROtT"
      },
      "source": [
        "### Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qizw17byRSY9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_numpy, y_numpy, test_size=0.2, random_state=13)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V44dlspTEz0"
      },
      "source": [
        "### Convert Back to Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgIjTvL4TAKG"
      },
      "outputs": [],
      "source": [
        "X_train_tensors = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "X_test_tensors = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "y_train_tensors = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
        "y_test_tensors = tf.convert_to_tensor(y_test, dtype=tf.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JchqmHB3rZyh"
      },
      "source": [
        "### Enhance Model Architecture\n",
        "Increasing the number of LSTM units and adding convolutional layers to better capture the complexities in the spectrograms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GWEsP3yrh3V"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Conv1D, Flatten, Reshape, Activation\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv1D(32, 3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(Conv1D(64, 3, activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Reshape((-1, 256)))\n",
        "model.add(LSTM(512, return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(y_train.shape[2])))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IppJR-XmRYXg"
      },
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rSqxy7BRXJC"
      },
      "outputs": [],
      "source": [
        "history1 = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eooH_UK0lZlY"
      },
      "source": [
        "### Generating audio files of predicted spectrograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5KOu8ojkGXO"
      },
      "outputs": [],
      "source": [
        "predicted_spectrograms = model.predict(X_test_tensors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8mE0BbhpSHd"
      },
      "outputs": [],
      "source": [
        "print(\"Shape of predicted_spectrograms:\", predicted_spectrograms.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dE98Tfin71Y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.interpolate import RegularGridInterpolator\n",
        "\n",
        "def resize_spectrogram(spectrogram, new_shape):\n",
        "    # Original dimensions\n",
        "    y = np.linspace(0, 1, spectrogram.shape[0])  # Frequency bins\n",
        "    x = np.linspace(0, 1, spectrogram.shape[1])  # Time frames\n",
        "\n",
        "    # Interpolation function\n",
        "    interpolating_function = RegularGridInterpolator((y, x), spectrogram)\n",
        "\n",
        "    # New dimensions\n",
        "    y_new = np.linspace(0, 1, new_shape[0])  # New frequency bins (1025)\n",
        "    x_new = np.linspace(0, 1, new_shape[1])  # New time frames\n",
        "\n",
        "    # New grid\n",
        "    y_new, x_new = np.meshgrid(y_new, x_new, indexing='ij')\n",
        "    new_grid = np.array([y_new, x_new]).reshape(2, -1).T\n",
        "\n",
        "    # Interpolate\n",
        "    new_spectrogram = interpolating_function(new_grid).reshape(new_shape)\n",
        "\n",
        "    return new_spectrogram\n",
        "\n",
        "# Assuming new_shape as (1025, 50)\n",
        "new_shape = (1025, 50)\n",
        "\n",
        "# Resizing each spectrogram\n",
        "resized_spectrograms = np.array([resize_spectrogram(spec, new_shape) for spec in predicted_spectrograms])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSjg6MkekG6F"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "# Function to convert spectrogram to audio\n",
        "def spectrogram_to_audio(spectrogram, n_fft, hop_length, num_iter=100):\n",
        "    # Convert dB to amplitude\n",
        "    spectrogram_amplitude = librosa.db_to_amplitude(spectrogram)\n",
        "\n",
        "    # Apply Griffin-Lim phase reconstruction\n",
        "    audio = librosa.griffinlim(spectrogram_amplitude, n_iter=num_iter, hop_length=hop_length, n_fft=n_fft)\n",
        "\n",
        "    return audio\n",
        "\n",
        "# Parameters for Griffin-Lim\n",
        "n_fft = 2048\n",
        "hop_length = 512\n",
        "\n",
        "# Generate audio for each predicted spectrogram\n",
        "generated_audios = [spectrogram_to_audio(spec, n_fft=n_fft, hop_length=hop_length) for spec in resized_spectrograms]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlQqIorrkRb1"
      },
      "outputs": [],
      "source": [
        "import soundfile as sf\n",
        "\n",
        "audio_output_2 = '/content/gdrive/MyDrive/Violin_Comp_Data/audio_output_2'\n",
        "\n",
        "for i, audio in enumerate(generated_audios):\n",
        "    output_file_path = audio_output_2 + f\"generated_audio_{i}.wav\"\n",
        "    sf.write(output_file_path, audio, samplerate=22050)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Music Evaluation\n",
        "Again, unfortunately, the audio is a bunch of white noise."
      ],
      "metadata": {
        "id": "w7ZoYToJNPhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experimenting with a WaveNet Model Architecture"
      ],
      "metadata": {
        "id": "EFbuSiLybPoV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HeY4K4t-2fS"
      },
      "source": [
        "### Preprocessing Audio Files & Sampling Dataset\n",
        "\n",
        "Converting each audio file in the dataset to a raw audio waveform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfpf5l6NOan6"
      },
      "outputs": [],
      "source": [
        "def select_random_files(source_directory, num_files_to_select):\n",
        "    all_files = [f for f in os.listdir(source_directory) if os.path.isfile(os.path.join(source_directory, f))]\n",
        "    selected_files = random.sample(all_files, num_files_to_select)\n",
        "    return selected_files\n",
        "\n",
        "def copy_files_to_directory(files, source_directory, destination_directory):\n",
        "    # Create the destination directory if it does not exist\n",
        "    if not os.path.exists(destination_directory):\n",
        "        os.makedirs(destination_directory)\n",
        "\n",
        "    # Copy each file to the destination directory\n",
        "    for file in files:\n",
        "        source_path = os.path.join(source_directory, file)\n",
        "        destination_path = os.path.join(destination_directory, file)\n",
        "        shutil.copy2(source_path, destination_path)\n",
        "\n",
        "# Source and destination directories\n",
        "source_directory = '/content/gdrive/MyDrive/Violin_Comp_Data/converted_music_files_cleaned'\n",
        "destination_directory = '/content/gdrive/MyDrive/Violin_Comp_Data/sample_music_files_150'\n",
        "\n",
        "# Number of files to select\n",
        "num_files_to_select = 150\n",
        "\n",
        "# Get the random sample of files\n",
        "random_sample_files = select_random_files(source_directory, num_files_to_select)\n",
        "\n",
        "# Copy the files\n",
        "copy_files_to_directory(random_sample_files, source_directory, destination_directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dytcZVMa-5KW"
      },
      "outputs": [],
      "source": [
        "def load_audio_files(directory, sampling_rate=22050):\n",
        "    audio_waveforms = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.wav'):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            # Load the audio file\n",
        "            audio, _ = librosa.load(file_path, sr=sampling_rate)\n",
        "            audio_waveforms.append(audio)\n",
        "    return audio_waveforms\n",
        "\n",
        "# Load and preprocess audio files\n",
        "audio_directory = '/content/gdrive/MyDrive/Violin_Comp_Data/sample_music_files_150'\n",
        "sampling_rate = 22050  # You can adjust this based on your data\n",
        "audio_waveforms = load_audio_files(audio_directory, sampling_rate=sampling_rate)\n",
        "\n",
        "# Flatten the list of waveforms to create one long waveform\n",
        "combined_waveform = np.concatenate(audio_waveforms)\n",
        "\n",
        "# Normalize the waveform between -1 and 1\n",
        "combined_waveform = combined_waveform / np.abs(combined_waveform).max()\n",
        "\n",
        "# Reshape for the model (model expects 3D input: samples, timesteps, 1)\n",
        "combined_waveform = combined_waveform.reshape(-1, 1, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y7GuBHnpQ3h"
      },
      "source": [
        "### Creating Data Generator Class\n",
        "Running out of RAM. A data generator creates data in batches on-the-fly during training, which significantly reduces memory usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNkjCTY6paXD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class WaveNetDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, data, sequence_length, batch_size):\n",
        "        self.data = data\n",
        "        self.sequence_length = sequence_length\n",
        "        self.batch_size = batch_size\n",
        "        self.indices = np.arange(len(data) - sequence_length - 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.indices) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        X = np.zeros((len(batch_indices), self.sequence_length, 1), dtype=np.float32)\n",
        "        y = np.zeros((len(batch_indices), 1), dtype=np.float32)\n",
        "\n",
        "        for i, idx in enumerate(batch_indices):\n",
        "            X[i] = self.data[idx:idx + self.sequence_length].reshape(-1, 1)\n",
        "            y[i] = self.data[idx + self.sequence_length]\n",
        "\n",
        "        return X, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WwgNBRfpqCd"
      },
      "source": [
        "### Instantiate and Use the Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93ZOh3dupuRx"
      },
      "outputs": [],
      "source": [
        "sequence_length = 4000\n",
        "batch_size = 32\n",
        "\n",
        "# Instantiate the data generator\n",
        "train_generator = WaveNetDataGenerator(combined_waveform, sequence_length, batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WUpdfZ_Bs8T"
      },
      "outputs": [],
      "source": [
        "# Check the generator's batch creation\n",
        "for X_batch, y_batch in train_generator:\n",
        "    print(\"Batch X shape:\", X_batch.shape)\n",
        "    print(\"Batch y shape:\", y_batch.shape)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uLFJltP9I3a"
      },
      "source": [
        "### WaveNet Model Structure:\n",
        "WaveNet uses dilated causal convolutions, which allow the network to have a very large receptive field with fewer layers. It also uses residual and skip connections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0W8EU57CCUuw"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Conv1D, Add, Activation, Multiply\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def residual_block(x, i, n_filters, filter_width, dilation_rate):\n",
        "    tanh_out = Conv1D(n_filters, filter_width, dilation_rate=dilation_rate, padding='same', activation='tanh')(x)\n",
        "    sigm_out = Conv1D(n_filters, filter_width, dilation_rate=dilation_rate, padding='same', activation='sigmoid')(x)\n",
        "    z = Multiply()([tanh_out, sigm_out])\n",
        "    skip = Conv1D(n_filters, 1)(z)\n",
        "    res = Add()([skip, x])\n",
        "    return res, skip\n",
        "\n",
        "def build_wavenet(input_shape, n_filters, filter_width, n_dilation_blocks):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    skip_connections = []\n",
        "    for i in range(n_dilation_blocks):\n",
        "        x, skip = residual_block(x, i, n_filters, filter_width, 2 ** i)\n",
        "        skip_connections.append(skip)\n",
        "    out = Add()(skip_connections)\n",
        "    out = Activation('relu')(out)\n",
        "    out = Conv1D(n_filters, 1, activation='relu')(out)\n",
        "    out = Conv1D(1, 1)(out)\n",
        "    model = Model(inputs=inputs, outputs=out)\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Model parameters\n",
        "input_shape = (None, 1)\n",
        "n_filters = 64\n",
        "filter_width = 2\n",
        "n_dilation_blocks = 6\n",
        "\n",
        "# Create the model\n",
        "wavenet_model = build_wavenet(input_shape, n_filters, filter_width, n_dilation_blocks)\n",
        "wavenet_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YptyV4BCCjXt"
      },
      "outputs": [],
      "source": [
        "sequence_length = 4000\n",
        "batch_size = 32\n",
        "\n",
        "train_generator = WaveNetDataGenerator(combined_waveform, sequence_length, batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOMCJ8PB_RRZ"
      },
      "source": [
        "### Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_Js0OpTERO-"
      },
      "outputs": [],
      "source": [
        "# Fit the model using the generator\n",
        "history3 = wavenet_model.fit(train_generator, epochs=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHns5u-O_S0I"
      },
      "outputs": [],
      "source": [
        "sequence_length = 4000  # Number of timesteps in each input sequence\n",
        "input_sequences = []\n",
        "target_samples = []\n",
        "\n",
        "for i in range(0, len(combined_waveform) - sequence_length):\n",
        "    input_sequences.append(combined_waveform[i:i+sequence_length])\n",
        "    target_samples.append(combined_waveform[i+sequence_length])\n",
        "\n",
        "input_sequences = np.array(input_sequences)\n",
        "target_samples = np.array(target_samples)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(input_sequences, target_samples, test_size=0.2, random_state=13)\n",
        "\n",
        "# Fit the model using the generator\n",
        "history3 = wavenet_model.fit(train_generator, epochs=5, validation_data=(X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFWiO5vg_N-u"
      },
      "source": [
        "### Generate Audio from the Model\n",
        "Adjusting with generating one sample at a time, which avoids complexity of batch predictions while still being more effecient than the original approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9a2isX0Ye_1"
      },
      "outputs": [],
      "source": [
        "def generate_audio_batch(model, seed_sequence, generation_length, sampling_rate, batch_size):\n",
        "    generated_audio = np.array(seed_sequence)\n",
        "\n",
        "    # Ensure seed_sequence is in the correct shape [1, sequence_length, 1]\n",
        "    seed_sequence = seed_sequence.reshape(1, -1, 1)\n",
        "\n",
        "    for i in range(generation_length):\n",
        "        # Predict the next sample and append it to generated_audio\n",
        "        next_sample = model.predict(seed_sequence)[0, -1, 0]\n",
        "        generated_audio = np.append(generated_audio, next_sample)\n",
        "\n",
        "        # Update seed_sequence to include the new sample\n",
        "        new_sample = np.array([[next_sample]])\n",
        "        seed_sequence = np.concatenate((seed_sequence[:, 1:, :], new_sample[:, :, np.newaxis]), axis=1)\n",
        "\n",
        "        # Break if enough samples are generated\n",
        "        if len(generated_audio) >= generation_length:\n",
        "            break\n",
        "\n",
        "    return generated_audio\n",
        "\n",
        "# Parameters\n",
        "sequence_length = 4000  # This should match the model's expected input size\n",
        "sampling_rate = 16000   # Replace with your actual sampling rate\n",
        "generation_length = sampling_rate * 10  # 10 seconds\n",
        "\n",
        "# Seed sequence to start generation\n",
        "seed_sequence = np.random.uniform(-1, 1, sequence_length)\n",
        "\n",
        "# Generate audio\n",
        "generated_audio = generate_audio_batch(wavenet_model, seed_sequence, generation_length, sampling_rate, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHMoM1k7_Wen"
      },
      "source": [
        "### Save the Generated Audio:\n",
        "You can save the generated audio to a file using librosa or soundfile."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwXY0vBT_UPX"
      },
      "outputs": [],
      "source": [
        "import soundfile as sf\n",
        "\n",
        "# Normalize the generated audio to between -1 and 1\n",
        "generated_audio = generated_audio / np.max(np.abs(generated_audio))\n",
        "\n",
        "# Save to a file\n",
        "sf.write('/content/gdrive/MyDrive/Violin_Comp_Data/WaveNet_generated_music/generated_audio.wav', generated_audio, samplerate=sampling_rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Music Evaluation\n",
        "Once again, the generated audio is a bunch of white noise. I'll adjust tactics by using MIDI files for model generation instead of spectrograms."
      ],
      "metadata": {
        "id": "nas7pcWpgcqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proceed to 'Model_1_WaveNet.ipynb'"
      ],
      "metadata": {
        "id": "g79YqZS4gueP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5XE4KXqxcAK_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}